{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd16f7f4",
   "metadata": {},
   "source": [
    "# Informations of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80549b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential libraries\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395577f",
   "metadata": {},
   "source": [
    "Load the data and Check the info of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4549ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('uci-secom.csv') #Load data and check the info of data \n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f165f",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(data.isnull().sum()) #check amount of columns containing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_na=data.isnull().sum()\n",
    "no_na.sort_values(ascending=False) #check which columns contain highest null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5858e",
   "metadata": {},
   "source": [
    "# Modified / Remove Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "typedata={str(k): len(list(v)) for k, v in data.groupby(data.dtypes, axis=1)} # check datatype for the datasets\n",
    "print(typedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc269a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bygroup=data.columns.to_series().groupby(data.dtypes).groups # check which columns belong to which datagroup\n",
    "print(bygroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ac36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Time',axis=1,inplace=True) # drop the categorical data column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e22ee",
   "metadata": {},
   "source": [
    "# Balance Datasets check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43545372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Pass/Fail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd75d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Pass/Fail'].replace(to_replace=[-1,1],value=[1,0],inplace=True) #replace value [-1,1] to [1,0] for [pass,fail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01945872",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:.3f}\".format(np.count_nonzero(data['Pass/Fail'])/float(data.shape[0]))) #check the ratio of pass to fail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # method which split dataset to training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Pass/Fail', axis=1) \n",
    "y = data[['Pass/Fail']] #target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42,stratify=y) #split X and y datasets to both train and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9212c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert train, testing datasets into pandas dataframe\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "y_train = pd.DataFrame(y_train, columns=y.columns)\n",
    "y_test = pd.DataFrame(y_test, columns=y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9adf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistics regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics used for evaluation\n",
    "from sklearn.metrics import f1_score,matthews_corrcoef\n",
    "\n",
    "# visualizations\n",
    "from yellowbrick.classifier import ClassPredictionError, ConfusionMatrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# KNN imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "\n",
    "# method used for handling imbalaced data\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3afe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values and save it as a temporary dataset\n",
    "imputer = KNNImputer()\n",
    "imputer.fit(X_train)\n",
    "imputed_train = pd.DataFrame(imputer.transform(X_train),columns = X_train.columns)\n",
    "imputed_test = pd.DataFrame(imputer.transform(X_test),columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da134a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "train_std = pd.DataFrame(scaler.fit_transform(imputed_train),columns = imputed_train.columns)\n",
    "test_std = pd.DataFrame(scaler.transform(imputed_test),columns = imputed_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling the dataset to balance the label '1' and label '0' in training datasets \n",
    "oversample = SMOTE(random_state = 2)\n",
    "over_X, over_y = oversample.fit_resample(train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce2423",
   "metadata": {},
   "source": [
    "# Selection of Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73719f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logisticRegression model with training datatsets\n",
    "logit = LogisticRegression(random_state = 42, class_weight = 'balanced', solver = 'liblinear',max_iter = 1000)\n",
    "logit.fit(over_X, over_y.values.ravel())\n",
    "\n",
    "# check training time required\n",
    "start_time = datetime.datetime.now()\n",
    "elapsed = datetime.datetime.now() - start_time\n",
    "time = int(elapsed.total_seconds()*1000)\n",
    "\n",
    "# check the F1 score and MCCscore of the logisticRegression model\n",
    "y_predict = logit.predict(test_std)\n",
    "y_true = y_test.values.ravel()\n",
    "f1score = f1_score(y_true, y_predict, average = 'micro')\n",
    "mccscore = matthews_corrcoef(y_true, y_predict)\n",
    "\n",
    "#visualizations of the result\n",
    "cpe = ClassPredictionError(logit,classes=['fail','pass']) #by Class\n",
    "cpe.score(test_std, y_true)\n",
    "cpe.show()\n",
    "\n",
    "cm = ConfusionMatrix(logit,classes=['fail','pass']) #by Confusion Matrix\n",
    "cm.score(test_std, y_true)\n",
    "cm.show()\n",
    "\n",
    "print (f' Training time: {time}ms\\n F1 Score: {f1score} \\n MCC Score: {mccscore}' )\n",
    "\n",
    "# create an empty list for F1scores, MCCscores and times \n",
    "f1scores = []\n",
    "mccscores =[]\n",
    "times =[]\n",
    "\n",
    "# append the result after each time the datasets were modified for comparison\n",
    "f1scores.append(f1score)\n",
    "mccscores.append(mccscore)\n",
    "times.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff471c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForestClassifier model with training datasets\n",
    "forest = RandomForestClassifier(class_weight = 'balanced',random_state = 42)\n",
    "forest.fit(over_X, over_y.values.ravel())\n",
    "\n",
    "# check training time required\n",
    "start_time = datetime.datetime.now()\n",
    "elapsed = datetime.datetime.now() - start_time\n",
    "time = int(elapsed.total_seconds()*1000)\n",
    "\n",
    "# check the F1 score and MCCscore of the model\n",
    "y_predict = forest.predict(test_std)\n",
    "y_true = y_test.values.ravel()\n",
    "f1score = f1_score(y_true, y_predict, average = 'micro')\n",
    "mccscore = matthews_corrcoef(y_true, y_predict)\n",
    "\n",
    "#visualizations of the result\n",
    "cpe = ClassPredictionError(forest,classes=['fail','pass']) # by Class\n",
    "cpe.score(test_std, y_true)\n",
    "cpe.show()\n",
    "\n",
    "cm = ConfusionMatrix(forest,classes=['fail','pass']) # by ConfusionMatrix\n",
    "cm.score(test_std, y_true)\n",
    "cm.show()\n",
    "\n",
    "print (f' Training time: {time}ms\\n F1 Score: {f1score} \\n MCC Score: {mccscore}' )\n",
    "\n",
    "# append result to their corresponding list\n",
    "f1scores.append(f1score)\n",
    "mccscores.append(mccscore)\n",
    "times.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop the result obtained from RandomForest Model as LogisticRegression model is more suitable for this dataset \n",
    "f1scores.pop()\n",
    "mccscores.pop()\n",
    "times.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to ease the training and testing process\n",
    "def evaluate(train_df, test_df, train_target, test_target):\n",
    "        \n",
    "        # data scaling for better perfomance of logistic regression\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df)\n",
    "        train_std = pd.DataFrame(scaler.transform(train_df), columns=train_df.columns)\n",
    "        test_std = pd.DataFrame(scaler.transform(test_df), columns = test_df.columns)\n",
    "        \n",
    "        # oversampling \n",
    "        oversample = SMOTE(random_state = 2)\n",
    "        over_X, over_y = oversample.fit_resample(train_std, train_target)\n",
    "        \n",
    "        # training the model\n",
    "        logit = LogisticRegression(random_state = 42, class_weight='balanced', solver='liblinear',max_iter = 1000)\n",
    "        logit.fit(over_X,over_y.values.ravel())\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        elapsed = datetime.datetime.now() - start_time\n",
    "        time = int(elapsed.total_seconds()*1000)\n",
    "        \n",
    "        y_predict = logit.predict(test_std)\n",
    "        y_true = test_target.values.ravel()\n",
    "        f1score = f1_score(y_true, y_predict, average = 'micro')\n",
    "        mccscore = matthews_corrcoef(y_true, y_predict)\n",
    "\n",
    "        #visualizations\n",
    "        cpe = ClassPredictionError(logit, classes=['fail','pass'])\n",
    "        cpe.score(test_std, y_true)\n",
    "        cpe.show()\n",
    "\n",
    "        cm = ConfusionMatrix(logit,classes=['fail','pass'])\n",
    "        cm.score(test_std, y_true)\n",
    "        cm.show()\n",
    "\n",
    "        return time, f1score, mccscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171b4fe",
   "metadata": {},
   "source": [
    "# 1. Eliminate Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns which is having missing values greater than threshold\n",
    "def percentna(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isna().sum()/dataframe.shape[1])>threshold] \n",
    "    return columns.tolist()\n",
    "na_columns = percentna(X_train, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebefc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the respective columns\n",
    "X_train_nona = X_train.drop(na_columns,axis = 1)\n",
    "X_test_nona = X_test.drop(na_columns,axis = 1)\n",
    "n_features1 = X_train_nona.shape[1]\n",
    "print(f' There are {n_features1} features left.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065cc5bd",
   "metadata": {},
   "source": [
    "# 2. Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values for those columns which having missing values less than threshold\n",
    "imputer = KNNImputer()\n",
    "X_train_imp = pd.DataFrame(imputer.fit_transform(X_train_nona), columns = X_train_nona.columns)\n",
    "X_test_imp = pd.DataFrame(imputer.transform(X_test_nona), columns = X_test_nona.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training time, F1score and MCCscore again after the missing values were all modified\n",
    "time,f1score,mccscore = evaluate(train_df = X_train_imp, test_df = X_test_imp, train_target = y_train, test_target = y_test)\n",
    "print (f' Training time: {time}ms\\n F1 Score: {f1score} \\n MCC Score: {mccscore}' )\n",
    "\n",
    "f1scores.append(f1score)\n",
    "mccscores.append(mccscore)\n",
    "times.append(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c48fb",
   "metadata": {},
   "source": [
    "# 3. Eliminate Variation below threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e380c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize the training and testing datasets\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train_imp)\n",
    "\n",
    "X_train_nrm = pd.DataFrame(normalizer.transform(X_train_imp), columns = X_train_imp.columns)\n",
    "X_test_nrm = pd.DataFrame(normalizer.transform(X_test_imp), columns = X_test_imp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce534f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set VarianceThreshold and Remove the features which having Variance Threshold = 0.0\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(X_train_nrm)\n",
    "\n",
    "# check which columns having variance threshold > 0.0\n",
    "mask = selector.get_support()  \n",
    "selected_cols = X_train_nrm.columns[mask]\n",
    "print(selected_cols)\n",
    "\n",
    "# transform the datasets\n",
    "X_train_var = pd.DataFrame(selector.transform(X_train_imp), columns = selected_cols)\n",
    "X_test_var = pd.DataFrame(selector.transform(X_test_imp),columns = selected_cols)\n",
    "\n",
    "#check quantity of remained features\n",
    "print(f' Remaining Features: {X_train_var.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model after normalization\n",
    "time,f1score,mccscore = evaluate(train_df = X_train_var, test_df = X_test_var, train_target = y_train, test_target = y_test)\n",
    "print (f' Training time: {time}ms\\n F1 Score: {f1score} \\n MCC Score: {mccscore}' )\n",
    "\n",
    "f1scores.append(f1score)\n",
    "mccscores.append(mccscore)\n",
    "times.append(time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7380df2",
   "metadata": {},
   "source": [
    "# 4. Pairwise Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a91ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which return first feature that is correlated with anything other feature\n",
    "# as example: if A is highly correlated with B, A is removed but B is retained \n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  \n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]  \n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a21ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the selected features\n",
    "corr_features = correlation(X_train_var, 0.95)\n",
    "X_train_corr = X_train_var.drop(corr_features, axis = 1)\n",
    "X_test_corr = X_test_var.drop(corr_features, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75148be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After removing {len(corr_features)}, there is {X_train_corr.shape[1]} remaining features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e96993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model after removing features with high pairwise correlation\n",
    "time,f1score,mccscore = evaluate(train_df = X_train_corr, test_df = X_test_corr, train_target = y_train, test_target = y_test)\n",
    "print (f' Training time: {time}ms\\n F1 Score: {f1score} \\n MCC Score: {mccscore}' )\n",
    "\n",
    "f1scores.append(f1score)\n",
    "mccscores.append(mccscore)\n",
    "times.append(time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728f337",
   "metadata": {},
   "source": [
    "# 5. Correlation with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a544d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # create a temporary datasets which include target columns\n",
    "temporary_train = X_train_corr.copy()\n",
    "temporary_train['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation of features with target\n",
    "temporary_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2ea17",
   "metadata": {},
   "source": [
    "Notice that some of the data is not available, and it may possibly caused by the variance of the features is 0.\n",
    "Let's check the data under those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the columns which having NA correlation to target as null_columns\n",
    "null = pd.DataFrame(temporary_train.corr())\n",
    "null_columns = null[null['target'].isna()]\n",
    "null_columns = null_columns.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ac3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for features correlation with target\n",
    "def corrwith_target(dataframe, target, threshold):\n",
    "    cor = dataframe.corr()\n",
    "    # Check the correlation value of each features with output variable\n",
    "    cor_target = abs(cor[target])\n",
    "    # Select lowly correlated features as irrelevant_features\n",
    "    irrelevant_features = cor_target[cor_target < threshold]\n",
    "    return irrelevant_features.index.tolist()[:-1] # return all irrelevant features but excluded the target itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a344aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorrwith_cols = corrwith_target(temporary_train, 'target', 0.05)\n",
    "# append the null_columns in the list of irrelevant_features\n",
    "for i in range(len(null_columns)):\n",
    "    wcorrwith_cols.append(null_columns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all irrelevant_features\n",
    "X_train_corw = X_train_corr.drop(wcorrwith_cols, axis = 1)\n",
    "X_test_corw = X_test_corr.drop(wcorrwith_cols, axis = 1)\n",
    "print(f' After removing {len(wcorrwith_cols)} features, there are {X_train_corw.shape[1]} features left.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model after removal of irrelevant features\n",
    "time,f1score,mccscore = evaluate(train_df = X_train_corr, test_df = X_test_corr, train_target = y_train, test_target = y_test)\n",
    "print (f' Training time: {time}ms\\n F1 Score: {f1score} \\n MCC Score: {mccscore}' )\n",
    "\n",
    "f1scores.append(f1score)\n",
    "mccscores.append(mccscore)\n",
    "times.append(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e71f1",
   "metadata": {},
   "source": [
    "# 6. Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e84b82",
   "metadata": {},
   "source": [
    "Recursive feature elimination with cross-validation:\n",
    "\n",
    "Method to search for optimum features for model improvement\n",
    "Search a subset of features based on their importance and eliminate the features which are considered less significant\n",
    "It will stop when a certain number of features are left, or elimination of features no longer help the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the remaining datasets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_corw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = pd.DataFrame(scaler.transform(X_train_corw), columns = X_train_corw.columns)\n",
    "X_test_std = pd.DataFrame(scaler.transform(X_test_corw), columns = X_test_corw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8565c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import methods for Recursive Features Elimination\n",
    "from sklearn.metrics import make_scorer\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "rfecv = RFECV(estimator=LogisticRegression(random_state = 42, class_weight='balanced', dual=False, solver='liblinear'),\n",
    "              cv=StratifiedKFold(2),\n",
    "              scoring =  mcc_scorer)\n",
    "rfecv.fit(X_train_std, y_train.values.ravel())\n",
    "rfecv.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6225712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the selected optimum columns from Recursive Feature Elimination\n",
    "mask = rfecv.get_support()\n",
    "columns = X_train_corw.columns\n",
    "selected_cols = columns[mask]\n",
    "\n",
    "X_train_rfe = pd.DataFrame(rfecv.transform(X_train_corw), columns = selected_cols)\n",
    "X_test_rfe = pd.DataFrame(rfecv.transform(X_test_corw), columns = selected_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4755c",
   "metadata": {},
   "source": [
    "Now let's check our evaluation model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time, f1score, mccscore = evaluate(train_df = X_train_rfe, test_df = X_test_rfe, train_target=y_train, test_target=y_test)\n",
    "\n",
    "print(f' Training time: {time}ms\\n F1 Score: {f1score}\\n MCC Score: {mccscore}')\n",
    "\n",
    "f1scores.append(f1score)\n",
    "mccscores.append(mccscore)\n",
    "times.append(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67d00d",
   "metadata": {},
   "source": [
    "# Overall Figure for Scores and Training Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b936dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Perfomance of model after each steps of features elimination\n",
    "fig, (ax0, ax1) = plt.subplots(2,1)\n",
    "ax0.plot(times, label = 'Training Time')\n",
    "ax0.set(ylabel = 'Training Time (ms)')\n",
    "ax1.plot(f1scores, label = 'F1 Score', c = 'green')\n",
    "ax1.plot(mccscores, label = 'MCC Score', c = 'red')\n",
    "ax1.set(ylabel = 'Score')\n",
    "ax1.set(xlabel = 'Feature selection step')\n",
    "ax1.legend()\n",
    "ax0.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e309f2",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "Successfully eliminated about 574 features features from 600 features and training time boost from 1338 ms to 8 ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
